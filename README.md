## 1. 개요
MNIST 및 CIFAR-10 데이터셋을 이용하여 MLP(Multi-Layer Perceptron)와 CNN(Convolutional Neural Network)의 구조적 차이와 이미지 분류 성능을 비교하는 것을 목표로 한다.

### 1-1. 데이터셋
|데이터셋|크기 (픽셀)|채널 수|특징|
|--|--|--|--|
|MNIST|28 * 28|1 (BW)|손글씨 숫자 이미지|
|CIFAR-10|32 * 32|3 (RGB)|RGB 컬러 이미지|

## 2. 구조
### 2-1. MLP (MNIST, CIFAR-10)
1. 전처리 과정을 통해 입력층을 28 * 28 = 784차원의 벡터로 입력받아 분류를 시작한다. (입력층)\
2. Dense(256, ReLU)
3. Dense(128, ReLU)
4. Dense(10, Softmax) (출력층)

MNIST와 같이 흑백 이미지에서는 높은 정확도를 얻었지만, CIFAR-10과 같은 컬러 이미지에서는 공간적 정보가 손실되어 약 40~50% 수준의 정확도에 머물렀다.

파라미터 변경\
은닉층 증가 시 : 데이터 정확도는 상승하지만, 과적합이 발생한다.\
학습률 증가 시 : 학습이 불안정해지고, Loss 그래프가 진동한다.

각 계층의 파라미터\
커널 사이즈: 해당 없음 (Dense layer 사용)\
커널 갯수: 해당 없음\
사용한 학습률: 0.001\
사용한 목적함수: Categorical Crossentropy\
사용한 업데이트 알고리즘: Adam


### 2-2. CNN (MNIST)
1. Conv2D(32, 3 * 3, ReLU)
2. MaxPooling2D(2 * 2)
3. Conv2D(64, 3 * 3, ReLU)
4. MaxPooling2D(2 * 2)
5. Flatten()
6. Dense(64, ReLU)
7. Dense(10, Softmax)

CNN에서는 MLP보다 정확도가 높다. 일반화 성능이 좋았다. 

파라미터 변경 실험\
커널 수 증가 시 : 정확도는 상승하지만, 학습 속도가 느려졌다.\
학습률 감소 시 : 수렴은 느리지만 더 안정적이다.\
각 계층의 파라미터\
커널 사이즈: 3 * 3\
커널 갯수: 32, 64\
사용한 학습률: 0.001\
사용한 목적함수: Categorical Crossentropy\
사용한 업데이트 알고리즘: Adam


### 2-3. CNN (CIFAR-10)
1. Conv2D(32, 3 * 3, ReLU)
2. MaxPooling2D(2 * 2)
3. Conv2D(64, 3 * 3, ReLU)
4. MaxPooling2D(2 * 2)
5. Flatten()
6. Dense(128, ReLU)
7. Dense(10, Softmax)

CIFAR-10은 MLP에서는 성능이 낮았지만, 다층 합성곱 계층을 사용하여 복잡한 특징을 학습하여 정확도가 보다 높았다.

파라미터 변경 실험\
커널 크기 증가 시 : 더 넓은 영역의 특징을 캐치하지만, 학습 속도가 느려진다. 연산량이 늘어났기 때문이다.

각 계층의 파라미터\
커널 사이즈: 3 * 3\
커널 갯수: 32, 64\
사용한 학습률: 0.001\
사용한 목적함수: Categorical Crossentropy\
사용한 업데이트 알고리즘: Adam

## 3. Softmax
Softmax는 모델의 출력 벡터를 확률 분포 형태로 변환하는 함수이다. 각 클래스의 점수를 지수화한 뒤, 전체 합으로 나누어 `0~1` 사이의 값으로 만든다. 이때 각 클래스 확률의 합은 항상 `1`이 되어, “모델이 각 클래스를 얼마나 확신하는가”를 의미한다.\
`x`에서 `np.max(x)`를 빼는 이유는 오버플로우를 방지하기 위함이다.

## 4. CrossEntropy
Cross Entropy는 실제 정답 분포(`y`)와 예측 확률 분포(`ŷ`) 사이의 차이를 계산하는 손실 함수다. 정답 클래스의 예측 확률이 높을수록 손실이 작고, 예측이 틀릴수록 `log` 에 의해 손실이 커진다. `log(0)`을 시도하면 손실이 무한대로 커지거나 학습이 멈추기 때문에 아주 작은 수인 `1e-9`를 더해 계산을 안전하게 한다.

Softmax 결과에서 모델은 첫 번째 요소인 `2.0`을 가장 높은 예측으로 확신하고 있다. (0.65)
CrossEntropy 손실 함수의 결과는 `0`에 가까울수록 완벽하고, `1`에 가까울수록 틀렸거나 확신하지 않는다는 뜻이다. 여기서는 `0.4` 정도가 나왔기 때문에 조금 높은 확률로 확신하고 있다.
